{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
      ],
      "metadata": {
        "id": "f2vvgnFpHpID"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Frontier Model"
      ],
      "metadata": {
        "id": "5YM5yOibLLPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = \"gpt-4.1\"  ### Frontier Model"
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sign in to OpenAI using Secrets in Colab\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "qP6OB2OeGC2C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that generate a synthetic dataset using the given instructions\"\n",
        "\n",
        "def user_prompt_message(user_message):\n",
        "  user_prompt = f\"Below is the instruction for the synthetic dataset. {user_message} Please create a synthetic dataset using the given data\"\n",
        "  return user_prompt"
      ],
      "metadata": {
        "id": "piEMmcSfMH-O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_assitance(user_message, history):\n",
        "\n",
        "  prompt = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": user_prompt_message(user_message)}]\n",
        "\n",
        "  response = openai.chat.completions.create(\n",
        "    model = gpt_model,\n",
        "    messages = prompt,\n",
        "    stream = True\n",
        "  )\n",
        "\n",
        "  result = \"\"\n",
        "  for chunk in response:\n",
        "    result += chunk.choices[0].delta.content or \"\"\n",
        "    yield result"
      ],
      "metadata": {
        "id": "E3L81ZRROCIA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(fn = chat_assitance, type = \"messages\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "T9z6wUn3OkBg",
        "outputId": "69a48e1a-c39e-4427-d93f-402b37e7c0d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3a1bac6b282838143e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3a1bac6b282838143e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Open Source Model"
      ],
      "metadata": {
        "id": "kLRfNL3EUf2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  ### Open Source Model"
      ],
      "metadata": {
        "id": "JxGqR3pJUoCK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sign in to HuggingFace Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "V0_zu9oGUn_z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Quantization\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "I8-VqgzwUn6m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_outputs(user_message, history):\n",
        "    prompt = [{\"role\": \"system\", \"content\": system_message}] + history + [\n",
        "        {\"role\": \"user\", \"content\": user_prompt_message(user_message)}\n",
        "    ]\n",
        "\n",
        "    LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    yield [{\"role\": \"assistant\", \"content\": response}]"
      ],
      "metadata": {
        "id": "roF1roU-aUe0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = []\n",
        "# print(model_outputs(\"Hello, how are you?\", history))"
      ],
      "metadata": {
        "id": "5Dno1_gRcX-h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(fn = model_outputs, type = \"messages\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "6jFwriE8Z9jH",
        "outputId": "dc61a42c-5173-4d36-961b-c9544e951947"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://119747bcc849780afa.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://119747bcc849780afa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wP8qk64FZhRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}